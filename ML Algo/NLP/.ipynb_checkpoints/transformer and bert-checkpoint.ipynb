{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.5.1-py3-none-any.whl (2.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.1 MB 345 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /home/dd/.local/lib/python3.8/site-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: packaging in /home/dd/anaconda3/lib/python3.8/site-packages (from transformers) (20.4)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
      "\u001b[K     |████████████████████████████████| 895 kB 5.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: filelock in /home/dd/anaconda3/lib/python3.8/site-packages (from transformers) (3.0.12)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.2-cp38-cp38-manylinux2010_x86_64.whl (3.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3 MB 4.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /home/dd/anaconda3/lib/python3.8/site-packages (from transformers) (4.50.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/dd/anaconda3/lib/python3.8/site-packages (from transformers) (2020.10.15)\n",
      "Requirement already satisfied: requests in /home/dd/anaconda3/lib/python3.8/site-packages (from transformers) (2.24.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/dd/anaconda3/lib/python3.8/site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: six in /home/dd/.local/lib/python3.8/site-packages (from packaging->transformers) (1.15.0)\n",
      "Requirement already satisfied: joblib in /home/dd/.local/lib/python3.8/site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: click in /home/dd/anaconda3/lib/python3.8/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/dd/anaconda3/lib/python3.8/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/dd/anaconda3/lib/python3.8/site-packages (from requests->transformers) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/dd/anaconda3/lib/python3.8/site-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/dd/anaconda3/lib/python3.8/site-packages (from requests->transformers) (3.0.4)\n",
      "Installing collected packages: sacremoses, tokenizers, transformers\n",
      "Successfully installed sacremoses-0.0.45 tokenizers-0.10.2 transformers-4.5.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoModel,BertTokenizerFast\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      0  Go until jurong point, crazy.. Available only ...\n",
       "1      0                      Ok lar... Joking wif u oni...\n",
       "2      1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      0  U dun say so early hor... U c already then say...\n",
       "4      0  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('bert.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.865937\n",
       "1    0.134063\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPplitting into vtrain,test and validation data set\n",
    "\n",
    "train_test_split?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_temp, y_train, y_temp = train_test_split(df['text'], df['label'], test_size=0.3, random_state=42,stratify=df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val,X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=2021)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert=AutoModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer=BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=['this is a bert model tutorial','we will have a good accuracy model']\n",
    "send_id=tokenizer.batch_encode_plus(text,padding=True,return_token_type_ids=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 2023, 2003, 1037, 14324, 2944, 14924, 4818, 102], [101, 2057, 2097, 2031, 1037, 2204, 10640, 2944, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "send_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# encode text sentence\n",
    "\n",
    "seq_len=[len(i.split()) for i in X_train]\n",
    "pd.Series(seq_len).hist(bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dd/anaconda3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "max_len_seq=25\n",
    "tokens_train=tokenizer.batch_encode_plus(\n",
    "X_train.to_list(),\n",
    "max_length=max_len_seq,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_seq=25\n",
    "tokens_test=tokenizer.batch_encode_plus(\n",
    "X_test.to_list(),\n",
    "max_length=max_len_seq,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_seq=25\n",
    "tokens_val=tokenizer.batch_encode_plus(\n",
    "X_val.to_list(),\n",
    "max_length=max_len_seq,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert integer sequences  to tensors\n",
    "\n",
    "train_seq=torch.tensor(tokens_train['input_ids'])\n",
    "train_mask=torch.tensor(tokens_train['attention_mask'])\n",
    "train_y=torch.tensor(y_train.to_list())\n",
    "\n",
    "val_seq=torch.tensor(tokens_val['input_ids'])\n",
    "val_mask=torch.tensor(tokens_val['attention_mask'])\n",
    "val_y=torch.tensor(y_val.to_list())\n",
    "\n",
    "test_seq=torch.tensor(tokens_test['input_ids'])\n",
    "test_mask=torch.tensor(tokens_test['attention_mask'])\n",
    "test_y=torch.tensor(y_test.to_list())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Data Loaders\n",
    "\n",
    "from torch.utils.data import TensorDataset,DataLoader,RandomSampler,SequentialSampler\n",
    "\n",
    "batch_size=12\n",
    "\n",
    "train_data=TensorDataset(train_seq,train_mask,train_y)\n",
    "\n",
    "train_sampler=RandomSampler(train_data)\n",
    "\n",
    "train_data_loader=DataLoader(train_data,sampler=train_sampler,batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "val_data=TensorDataset(val_seq,val_mask,val_y)\n",
    "\n",
    "val_sampler=RandomSampler(val_data)\n",
    "\n",
    "val_data_loader=DataLoader(val_data,sampler=val_sampler,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## freezing bert para,etrs\n",
    "for param in bert.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_Arch:\n",
    "\n",
    "    def __init__(self, bert):\n",
    "        super(BERT_Arch, self).__init__()\n",
    "\n",
    "        self.bert = bert \n",
    "\n",
    "          # dropout layer\n",
    "        self.dropout = bert.Dropout(0.1)\n",
    "\n",
    "          # relu activation function\n",
    "        self.relu = bert.ReLU()\n",
    "\n",
    "          # dense layer 1\n",
    "        self.fc1 = bert.Linear(768,512)\n",
    "\n",
    "          # dense layer 2 (Output layer)\n",
    "        self.fc2 = bert.Linear(512,2)\n",
    "\n",
    "          #softmax activation function\n",
    "        self.softmax = bert.LogSoftmax(dim=1)\n",
    "\n",
    "    #define the forward pass\n",
    "    def forward(self, sent_id, mask):\n",
    "\n",
    "      #pass the inputs to the model  \n",
    "        _, cls_hs = self.bert(sent_id, attention_mask=mask)\n",
    "\n",
    "        x = self.fc1(cls_hs)\n",
    "\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "          # output layer\n",
    "        x = self.fc2(x)\n",
    "\n",
    "          # apply softmax activation\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BertModel' object has no attribute 'Dropout'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-72e6e74d4fd3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# pass the pre-trained BERT to our define architecture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBERT_Arch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-33-c3c089a0376b>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, bert)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m           \u001b[0;31m# dropout layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m           \u001b[0;31m# relu activation function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    945\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0m\u001b[1;32m    948\u001b[0m             type(self).__name__, name))\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BertModel' object has no attribute 'Dropout'"
     ]
    }
   ],
   "source": [
    "# pass the pre-trained BERT to our define architecture\n",
    "model = BERT_Arch(bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
